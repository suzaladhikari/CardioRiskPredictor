{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Libraries to be imported \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score, mean_squared_error,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data to be used\n",
    "data = pd.read_csv('forModel.csv')\n",
    "data.drop(['Checkup','Unnamed: 0.1','Unnamed: 0'], axis = 1, inplace =True)\n",
    "\n",
    "##Cloned Data \n",
    "clonedData = pd.read_csv('forModel.csv')\n",
    "clonedData['Heart_Disease'] = clonedData['Heart_Disease'].map({'Yes':1,'No':0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation ! \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 160000 rows and 19 columns\n",
      "General_Health                   object\n",
      "Exercise                         object\n",
      "Heart_Disease                    object\n",
      "Skin_Cancer                      object\n",
      "Other_Cancer                     object\n",
      "Depression                       object\n",
      "Diabetes                         object\n",
      "Arthritis                        object\n",
      "Sex                              object\n",
      "Age_Category                     object\n",
      "Height_(cm)                     float64\n",
      "Weight_(kg)                     float64\n",
      "BMI                             float64\n",
      "Smoking_History                  object\n",
      "Alcohol_Consumption             float64\n",
      "Fruit_Consumption               float64\n",
      "Green_Vegetables_Consumption    float64\n",
      "FriedPotato_Consumption         float64\n",
      "SexBinary                         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## lets first evaluate the datas that we will be using ! \n",
    "\n",
    "print(f\"The data has {data.shape[0]} rows and {data.shape[1]} columns\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique features that the columns General_Health has is 5\n",
      "The number of unique features that the columns Exercise has is 2\n",
      "The number of unique features that the columns Heart_Disease has is 2\n",
      "The number of unique features that the columns Skin_Cancer has is 2\n",
      "The number of unique features that the columns Other_Cancer has is 2\n",
      "The number of unique features that the columns Depression has is 2\n",
      "The number of unique features that the columns Diabetes has is 4\n",
      "The number of unique features that the columns Arthritis has is 2\n",
      "The number of unique features that the columns Sex has is 2\n",
      "The number of unique features that the columns Age_Category has is 13\n",
      "The number of unique features that the columns Smoking_History has is 2\n"
     ]
    }
   ],
   "source": [
    "## Lets convert few columns in order to create few \n",
    "##Lets use the One Hot Encoder to change all the categorical data into the numerical datatypes \n",
    "onlyCategorical = data.select_dtypes(\"object\")\n",
    "for everything in onlyCategorical.columns:\n",
    "    print(f\"The number of unique features that the columns {everything} has is {data[everything].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As seen lets convert the binary category using the .map and nominal category using the onehotencoding \n",
    "data['General_Health'] = data['General_Health'].apply(lambda x: 'Poor' if x == 'Poor' else 'Good')\n",
    "data['General_Health'] = data['General_Health'].map({\"Poor\":1, \"Good\":0})\n",
    "data['Sex'] = data['Sex'].map({'Male':1, 'Female':0})\n",
    "data['Diabetes'] = data['Diabetes'].apply(lambda x:'No' if x == 'No' else 'Yes')\n",
    "## Since most of the Binary have Yes and No we will convert all of them at once \n",
    "cols = ['Exercise','Heart_Disease','Skin_Cancer','Other_Cancer','Depression','Arthritis','Smoking_History','Diabetes']\n",
    "for each in cols:\n",
    "    data[each] = data[each].map({'Yes':1,'No':0}) ##This converts the whole data into the binary \n",
    "\n",
    "dummiedData = pd.get_dummies(data, columns=['Age_Category'], dtype = 'int')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reason to convert the binary categories using the .map instead of using OneHotEncoding is to minimize the number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets fit and transform the data to simple imputer in order to avoid any null values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputed = imputer.fit_transform(dummiedData)\n",
    "imputedDataFrame = pd.DataFrame(imputed, columns =dummiedData.columns)\n",
    "\n",
    "\n",
    "## Lets use the Standard Scalar \n",
    "standard = StandardScaler()\n",
    "scaled = standard.fit_transform(imputedDataFrame)\n",
    "scaledData = pd.DataFrame(scaled, columns=imputedDataFrame.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaledData.drop('Heart_Disease', axis = 1).values \n",
    "y = clonedData['Heart_Disease'].values\n",
    "X_train,X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state=32)\n",
    "\n",
    "dimensionReduction = PCA(n_components=0.95) ## Our goal is to achieve the 95% of the variance of the data\n",
    "X_train_pca = dimensionReduction.fit_transform(X_train)\n",
    "PC1 = X_train_pca[:,0]\n",
    "PC2 = X_train_pca[:,1]\n",
    "X_test_pca = dimensionReduction.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C= 10) ## Model we will be using \n",
    "logit.fit(X_train_pca,y_train)\n",
    "y_pred = logit.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11533  4244]\n",
      " [ 4464 11759]]\n",
      "The accuracy of the model is 73.0 %\n",
      "The precision of the model is 73.0 %\n",
      "The recall of the model is 72.0 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73     15997\n",
      "           1       0.72      0.73      0.73     16003\n",
      "\n",
      "    accuracy                           0.73     32000\n",
      "   macro avg       0.73      0.73      0.73     32000\n",
      "weighted avg       0.73      0.73      0.73     32000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Lets check the model's performance manually\n",
    "confusionMatrix = confusion_matrix(y_pred, y_test)\n",
    "TrueNegative = confusionMatrix[0][0]\n",
    "FalsePositive = confusionMatrix[0][1]\n",
    "FalseNegative = confusionMatrix[1][0]\n",
    "TruePositive = confusionMatrix[1][1]\n",
    "print(confusionMatrix)\n",
    "accuracy = (TruePositive+TrueNegative)/(TruePositive+TrueNegative+FalsePositive+FalseNegative)\n",
    "print(f\"The accuracy of the model is {np.round(accuracy,2) * 100} %\")\n",
    "\n",
    "precision = (TruePositive)/(TruePositive+FalsePositive)\n",
    "print(f\"The precision of the model is {np.round(precision,2) * 100} %\")\n",
    "recall = (TruePositive)/(TruePositive+FalseNegative)\n",
    "print(f\"The recall of the model is {np.round(recall,2) * 100} %\")\n",
    "\n",
    "## alternative way \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n"
     ]
    }
   ],
   "source": [
    "## Lets do the hyperparameter tuning to figure out which parameter is perfect ?!\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "grid = GridSearchCV(logit,param_grid, cv = 5)\n",
    "grid.fit(X_train_pca,y_train)\n",
    "print(grid.best_params_) ## The best value for C has been updated "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
